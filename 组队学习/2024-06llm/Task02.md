# 第一章 使用LLM API开发应用



## Prompt

​		在人工智能和自然语言处理（NLP）领域，"prompt" 是指提供给语言模型的输入文本，旨在引导模型生成特定类型的输出。Prompt 是与大语言模型（如 GPT-3、GPT-4 等）交互的主要方式，通过设计不同的 prompt，用户可以获取所需的回答、文本生成或其他任务的解决方案。

## Temperature

​		在大型语言模型（LLM）开发中，"temperature" 是一个超参数，用于控制生成文本的随机性和创造性。这一参数影响模型在生成过程中对不同可能性选项的选择倾向。

### Temperature 的作用

1. **控制随机性**：
   - **低温度**（接近 0）：生成的文本更确定和保守，模型更倾向于选择最高概率的单词，输出的内容更加一致和可预测。这适用于需要准确和一致性的任务，如翻译或技术文档生成。
   - **高温度**（大于 1）：生成的文本更随机和多样，模型会更多地选择概率较低的单词，输出的内容更加多样化和创造性。这适用于需要创造性和想象力的任务，如写作或诗歌生成。
2. **调节探索与利用**：
   - Temperature 参数调节了模型对“探索”（选择不同的词）和“利用”（选择最可能的词）的平衡。低温度时，模型更多地利用已有的知识，高温度时，模型更多地探索可能的选项。

### Temperature 的实现

在生成文本时，语言模型通常会计算每个可能的下一个单词的概率分布。Temperature 参数通过以下公式调整这些概率：
$$
P_i^{′}= \frac{P_i^{1/T}}{\sum_{j} P_j^{1/T}}
$$
其中：

- p<sub>i</sub>是原始概率。
- T 是 temperature 参数。
- P<sub>i</sub><sup>′</sup>是调整后的概率。

​		当T取值较低接近 0 时，预测的随机性会较低，产生更保守、可预测的文本，不太可能生成意想不到或不寻常的词。当T取值较高接近 1 时，预测的随机性会较高，所有词被选择的可能性更大，会产生更有创意、多样化的文本，更有可能生成不寻常或意想不到的词。

## System Prompt

​		"System Prompt" 是一个用于控制大语言模型（LLM）行为的工具，在对话或生成文本任务中扮演着重要角色。它为模型提供背景信息、指导模型如何回应用户的输入，以及设定对话的整体语调和目标。具体来说，System Prompt 是一种隐藏的、预设的输入，通常由开发者或应用程序设计者设定，而不是由最终用户输入的。

### 示例

假设我们正在开发一个旅游助手应用，我们可以使用以下 System Prompt 来设定模型的行为和上下文：

```
你是一个旅游助手，负责为用户提供旅游建议和信息。请保持友好和专业的语气，提供详细的回答。不要讨论政治或宗教话题。
```

在这种情况下，当用户输入“请推荐一个适合家庭旅行的地方”时，模型会参考 System Prompt，并生成符合预设行为和语调的回答，例如：

```
当然！对于家庭旅行，我推荐去佛罗里达的迪士尼乐园。那里有各种适合孩子和成人的游乐设施和活动
```

### 其余见prompt.ipynb
